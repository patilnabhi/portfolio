---
layout: post
title:  "TurtleBot SLAM (with RTAB-Map, Hand-Gestures, Face Recognition & AR Code Tracking)"
date:   2016-03-14 13:00:00
last_modified_at:  2016-03-14 13:00:00
excerpt: ""
categories: project
tags:  turtlebot, hand gesture recognition, face recognition, fisherfaces, OpenCv, robotics
image:
  feature: tbot.png
  topPosition: 0px
bgContrast: dark
bgGradientOpacity: darker
syntaxHighlighter: no
---
<h4>Goal:</h4>

* Navigation of TurtleBot using hand-gestures to find target locations marked with AR codes and/or to find a specific person using face-recognition

<h4>Overview:</h4>

* This project consists of following **4 tasks**:  
<br>
1. Determine number of fingers (gestured) in front of an ASUS Xtion Pro camera
2. Navigate TurtleBot in an unknown environment using RGB-D SLAM approach, concurrently building a 3D map of the environment; the robot first finds a target station marked with AR code matching the number detected in (1) and then moves towards the target station
3. Capture, train and recognize faces of people in real-time using a simple GUI 
4. Move the robot to various stations to find a specific person from the database built in (3)  
<br>
* The overall execution of the project is summarised in the flow-chart below:

<center><img style="border: solid" src="{{ site.baseurl }}/assets/images/posts/tbotnav/flow_chart.png" width="100%"/></center>

<h4>Demo:</h4>  

<p><center><iframe width="1920" height="1080" src="https://www.youtube.com/embed/u7TcyaLekFg" frameborder="0" allowfullscreen></iframe></center></p>

<h4>Project Website:</h4>  

* A tutorial on launching the project with your TurtleBot is available on GitHub - [http://github.com/patilnabhi/tbotnav]

<h4>Project Details:</h4>

1. **[Hand-gesture recognition]:** Using a Kinect, [OpenCV] and [ROS], develop an API to recognize number of fingers from 0 to 5  
<br>
2. **[Face recognition]:** Using a webcam, OpenCV and ROS, develop an API to create a database of people's faces and recognize faces in real-time  
<br>
3. **[TurtleBot SLAM]:** Using TurtleBot, Kinect and ROS, implement RTAB-Map (a RGB-D SLAM approach) to navigate TurtleBot in an unknown environment  
<br>
<!-- 4. **AR code tracking:** Using the ar_track_alvar package in ROS and a Kinect, track AR codes to determine their 'ids' and 'poses' relative to a frame -->

<h4>Future work:</h4>
    
* **Object tracking:** Replace AR code tracking and get TurtleBot to find specific objects in the environment
* **RTAB-Map & beyond:** Explore the capabilities of RTAB-Map and RGB-D SLAM to make the navigation more robust
* **Simple is beautiful:** Improve the overall execution of the project to make it more user interactive by making it simpler/easier

*This project was completed as part of the MS in Robotics ([MSR]) program at Northwestern University ([NU]).*

<!-- **References:** -->


[http://github.com/patilnabhi/tbotnav]: http://github.com/patilnabhi/tbotnav
[ROS]: http://www.ros.org/
[NU]: http://www.mccormick.northwestern.edu/robotics/
[OpenCV]: http://opencv.org/
[Face recognition]: {{ site.baseurl }}face-recognition
[TurtleBot SLAM]: http://github.com/patilnabhi/tbotnav
[MSR]: http://www.mccormick.northwestern.edu/robotics/meet-students/profiles-2015-2016/patil-abhishek.html
[Hand-gesture recognition]: {{ site.baseurl }}hand-gestures